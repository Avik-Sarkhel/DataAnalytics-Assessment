# ğŸ“Š Data Analytics Assessment â€“ SQL

This repository contains SQL scripts used for analyzing **user behavior**, **transactions**, and **customer value** across a large-scale **financial dataset**. The objective was to demonstrate practical data analytics skills through **data cleaning**, **transformation**, and **business metric calculations**.

---

## ğŸ§  Project Overview

This SQL-based assessment tested my ability to work with a **complex, multi-table schema** involving financial products. The focus was on:

- Cleaning dirty or invalid data
- Segmenting users by behavioral traits
- Identifying inactive plans
- Calculating Customer Lifetime Value (CLV)

---

## ğŸ“ Files Included

| File Name                         | Description                                         |
|----------------------------------|-----------------------------------------------------|
| `Assessment Q1.sql`              | Question 1 â€“ Data Cleaning                         |
| `Assessment Q2.sql`              | Question 2 â€“ Frequency Segmentation                |
| `Assessment Q3.sql`              | Question 3 â€“ Inactive Plans Detection              |
| `Assessment Q4.sql`              | Question 4 â€“ Customer Lifetime Value Calculation   |
| `plans table data cleaning.sql`  | Cleaning logic for the `plans` table               |
| `savings table data cleaning.sql`| Cleaning logic for the `savings` table             |
| `user table data cleaning.sql`   | Cleaning logic for the `user` table                |
| `withdrawal table data cleaning.sql`| Cleaning logic for the `withdrawal` table       |
| `README.md`                      | This file â€“ project description and reflections    |

---

## ğŸ’¡ My Experience

This was my **first real SQL assessment**, and honestly, the schema was intimidating:

- `plans`: 56 columns  
- `savings`: 37 columns  
- `user`: 57 columns  
- `withdrawal`: 23 columns  

At first, I was stuck. But my â€œnever give upâ€ mindset kicked in. I didnâ€™t rush into writing queries. I **analyzed the schema**, **understood relationships**, and **documented everything** on pen and paper before diving in.

---

## ğŸ§¹ Data Cleaning Approach

Before writing any analytical queries, I cleaned each table **separately** using a **soft filtering** technique by adding an `is_deleted` flag:

- `0` â†’ Valid row  
- `1` â†’ Invalid / flagged row  

### ğŸ§¼ Cleaning Logic Summary

#### âœ… Plans Table
- Removed duplicate IDs  
- Checked nulls in foreign keys  
- Removed future/null start dates  
- Flagged 0 or negative investment amounts (based on `is_fund`)  
- Standardized boolean flags  
- Detected test/dummy data  
- Validated start/end date logic  

#### âœ… User Table
- Handled nulls in key fields  
- Flagged future `join_date` / `birth_date`  
- Removed dummy/test users  
- Flagged potential fraud-risk users  
- Invalid geo-coordinates handled  

#### âœ… Savings Table
- Removed future transaction/maturity dates  
- Flagged suspicious/negative savings  
- Skipped overly aggressive filtering to avoid data loss  
- Invalid/missing fees flagged  

#### âœ… Withdrawal Table
- Flagged nulls in key fields  
- Removed negative withdrawals  
- Handled invalid dates and dummy/test rows  
- Checked broken business logic  

> ğŸ§  The `savings` table was the **messiest**. Took the most time and taught me how critical clean data is.

---

## ğŸ“ Per-Question Explanations

### Q1: Data Cleaning
- Cleaned all 4 tables with a consistent `is_deleted` logic  
- Avoided hard deletes to preserve traceability

### Q2: Frequency Segmentation
- Grouped transactions by user  
- Calculated average monthly savings  
- Classified users into

