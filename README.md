# üìä Data Analytics Assessment ‚Äì SQL

This repository contains SQL scripts used for analyzing **user behavior**, **transactions**, and **customer value** across a large-scale **financial dataset**. The objective was to demonstrate practical data analytics skills through **data cleaning**, **transformation**, and **business metric calculations**.

---

## üß† Project Overview

This SQL-based assessment tested my ability to work with a **complex, multi-table schema** involving financial products. The focus was on:

- Cleaning dirty or invalid data  
- Segmenting users by behavioral traits  
- Identifying inactive plans  
- Calculating Customer Lifetime Value (CLV)

---

## üìÅ Files Included

| File Name                         | Description                                         |
|----------------------------------|-----------------------------------------------------|
| `Assessment Q1.sql`              | Question 1 ‚Äì Data Cleaning                         |
| `Assessment Q2.sql`              | Question 2 ‚Äì Frequency Segmentation                |
| `Assessment Q3.sql`              | Question 3 ‚Äì Inactive Plans Detection              |
| `Assessment Q4.sql`              | Question 4 ‚Äì Customer Lifetime Value Calculation   |
| `plans table data cleaning.sql`  | Cleaning logic for the `plans` table               |
| `savings table data cleaning.sql`| Cleaning logic for the `savings` table             |
| `user table data cleaning.sql`   | Cleaning logic for the `user` table                |
| `withdrawal table data cleaning.sql`| Cleaning logic for the `withdrawal` table       |
| `README.md`                      | This file ‚Äì project description and reflections    |

---

## üí° My Experience

This was my **first real SQL assessment**, and honestly, the schema was intimidating:

- `plans`: 56 columns  
- `savings`: 37 columns  
- `user`: 57 columns  
- `withdrawal`: 23 columns  

At first, I was stuck. But my ‚Äúnever give up‚Äù mindset kicked in. I didn‚Äôt rush into writing queries. I **analyzed the schema**, **understood relationships**, and **documented everything** on pen and paper before diving in.

---

## üßπ Data Cleaning Approach

Before writing any analytical queries, I cleaned each table **separately** using a **soft filtering** technique by adding an `is_deleted` flag:

- `0` ‚Üí Valid row  
- `1` ‚Üí Invalid / flagged row  

### üßº Cleaning Logic Summary

#### ‚úÖ Plans Table
- Removed duplicate IDs  
- Checked nulls in foreign keys  
- Removed future/null start dates  
- Flagged 0 or negative investment amounts (based on `is_fund`)  
- Standardized boolean flags  
- Detected test/dummy data  
- Validated start/end date logic  

#### ‚úÖ User Table
- Handled nulls in key fields  
- Flagged future `join_date` / `birth_date`  
- Removed dummy/test users  
- Flagged potential fraud-risk users  
- Invalid geo-coordinates handled  

#### ‚úÖ Savings Table
- Removed future transaction/maturity dates  
- Flagged suspicious/negative savings  
- Skipped overly aggressive filtering to avoid data loss  
- Invalid/missing fees flagged  

#### ‚úÖ Withdrawal Table
- Flagged nulls in key fields  
- Removed negative withdrawals  
- Handled invalid dates and dummy/test rows  
- Checked broken business logic  

> üß† The `savings` table was the **messiest**. Took the most time and taught me how critical clean data is.

---

## üìù Per-Question Explanations

### Q1: Data Cleaning
- Cleaned all 4 tables with a consistent `is_deleted` logic  
- Avoided hard deletes to preserve traceability

### Q2: Frequency Segmentation
- Grouped transactions by user  
- Calculated average monthly savings  
- Classified users into: **High**, **Medium**, and **Low Frequency**

### Q3: Inactive Plans
- Identified active/inactive savings plans  
- Plans with **no transaction in last 12+ months** were flagged

### Q4: Customer Lifetime Value (CLV)
- Calculated user tenure in months  
- Estimated avg monthly contribution  
- Used:  
  `CLV = (Monthly Transactions √ó 12) √ó Avg Profit Per Transaction`

---

## üöß Challenges & How I Tackled Them

| Challenge                         | How I Solved It                                        |
|----------------------------------|--------------------------------------------------------|
| Large schema + too many columns | Broke it into chunks, noted down structure manually   |
| Dirty/messy savings table        | Less aggressive filtering to avoid data loss          |
| Soft filtering via `is_deleted` | Helped me track what‚Äôs invalid without removing rows  |
| Financial domain unfamiliarity   | Used Google + ChatGPT + logic + common sense          |

---

## üõ†Ô∏è Tech Stack

- **SQL** ‚Äì MySQL-compatible syntax  
- **Pen & Paper** ‚Äì For breaking down schema  
- **GitHub** ‚Äì Version control and documentation

---

## üë®‚Äçüíª Author

**Avik Sarkhel**  
üéì MCA Student | Aspiring Data Analyst  
üìç Based in Kolkata, India  
üì´ [avik305sarkhel@gmail.com](mailto:avik305sarkhel@gmail.com)  
üîó [LinkedIn ‚Äì Avik Sarkhel](https://www.linkedin.com/in/avik-sarkhel)

---

## ‚ú® Final Words

This assessment wasn‚Äôt just a test ‚Äî it was a **level-up moment**.  
I learned:

- How much **cleaning** affects analysis  
- How to approach **complex schemas**  
- That SQL isn‚Äôt just about SELECTs, it‚Äôs about **logic** and **business thinking**

> üí° It‚Äôs okay to feel lost at first. Start small. Think clearly. The answers are always in the data.

---

> ‚≠ê If this project helped you or inspired your own SQL learning journey, drop a ‚≠ê and connect!
